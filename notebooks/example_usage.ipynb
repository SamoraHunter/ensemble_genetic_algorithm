{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Enable or disable GPU\n",
                "\n",
                "# #os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
                "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# import shutil\n",
                "# import os\n",
                "# directory = r'HFE_GA_experiments'\n",
                "\n",
                "# if os.path.exists(directory):\n",
                "#     shutil.rmtree(directory)\n",
                "#     print(f\"The directory '{directory}' has been successfully removed.\")\n",
                "# else:\n",
                "#     print(f\"The directory '{directory}' does not exist.\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import ipywidgets as ipw\n",
                "output = ipw.Output()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Setup logs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\n",
                "from ml_grid.util.logger_setup import setup_logger\n",
                "\n",
                "# Set up a basic logger for the initial cells\n",
                "logger = setup_logger()\n",
                "\n",
                "mpl_logger = logging.getLogger('matplotlib.font_manager')\n",
                "\n",
                "# Set the logging level to suppress debug messages\n",
                "mpl_logger.setLevel(logging.INFO)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Function to recursively remove a directory\n",
                "def remove_directory(path):\n",
                "    if os.path.exists(path):\n",
                "        for root, dirs, files in os.walk(path, topdown=False):\n",
                "            for name in files:\n",
                "                os.remove(os.path.join(root, name))\n",
                "            for name in dirs:\n",
                "                os.rmdir(os.path.join(root, name))\n",
                "        os.rmdir(path)\n",
                "        logger.info(f\"Directory '{path}' removed successfully.\")\n",
                "    else:\n",
                "        logger.info(f\"Directory '{path}' does not exist.\")\n",
                "\n",
                "# Specify the directory path to be removed\n",
                "directory_path = 'HFE_GA_experiments'\n",
                "\n",
                "# Remove the directory\n",
                "remove_directory(directory_path)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"Orchestrates a GA grid search and evaluates ensemble models.\n",
                "\n",
                "This script manages a complete pipeline for conducting a genetic algorithm (GA)\n",
                "grid search using the `ml_grid` framework. It automates feature selection,\n",
                "hyperparameter optimization, and the evaluation of various ensemble machine\n",
                "learning models for reproducible experiments.\n",
                "\n",
                "Attributes:\n",
                "    project_directory (str): The global path for saving all experiment outputs.\n",
                "    input_csv_path (str): The file path for the input dataset (CSV format).\n",
                "    n_iter (int): The total number of grid search iterations to perform.\n",
                "    modelFuncList (list): A list of model generator functions that serve as the\n",
                "        base learners for the ensemble methods.\n",
                "\n",
                "Workflow:\n",
                "    1.  **Initialization**:\n",
                "        - Imports necessary modules, including model generators (e.g., logistic\n",
                "          regression, random forest, XGBoost), utilities, and logging tools.\n",
                "        - Establishes a global project directory and creates a unique,\n",
                "          timestamped subdirectory for the current experiment run.\n",
                "        - Initializes a logger to capture and save experiment logs.\n",
                "\n",
                "    2.  **Configuration**:\n",
                "        - Sets the path to the input dataset CSV file.\n",
                "        - Initializes a `project_score_save_class` instance to log the\n",
                "          scores of each experiment iteration to a central CSV file.\n",
                "        - Defines the list of base learners (`modelFuncList`) and the number\n",
                "          of grid search iterations (`n_iter`).\n",
                "        - Instantiates a grid iterator (`grid_iter_obj`) to supply\n",
                "          hyperparameter combinations for each run.\n",
                "\n",
                "    3.  **Main Experiment Loop**:\n",
                "        - Iterates through the specified number of grid search trials.\n",
                "        - In each iteration, it fetches a new set of hyperparameters.\n",
                "        - An `ml_grid_object` is created to handle data loading, preprocessing,\n",
                "          and overall experiment configuration for the current trial.\n",
                "        - The core genetic algorithm pipeline (`main_ga.run().execute()`) is\n",
                "          executed to:\n",
                "            - Evolve ensembles of base learners.\n",
                "            - Evaluate each ensemble against the dataset.\n",
                "            - Log performance metrics and configurations to the experiment\n",
                "              directory.\n",
                "\n",
                "Key Features:\n",
                "    - Supports a wide range of base learners for flexible ensemble creation.\n",
                "    - Fully automates the hyperparameter search and model evaluation process.\n",
                "    - Organizes all results, logs, and artifacts in a structured,\n",
                "      timestamped directory to ensure reproducibility.\n",
                "    - Easily adaptable for both synthetic and real-world datasets.\n",
                "    - Modular design allows for the simple addition of new models or search\n",
                "      parameters.\n",
                "\n",
                "This pipeline is ideal for large-scale, systematic benchmarking of ensemble\n",
                "methods and GA-based feature selection strategies.\n",
                "\"\"\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import ml_grid\n",
                "import pathlib\n",
                "import datetime\n",
                "from tqdm import tqdm\n",
                "from ml_grid.model_classes_ga.dummy_model import DummyModelGenerator\n",
                "from ml_grid.util import grid_param_space_ga\n",
                "import pandas as pd\n",
                "\n",
                "from ml_grid.util.project_score_save import project_score_save_class\n",
                "from ml_grid.util.global_params import global_parameters\n",
                "\n",
                "# Initialize global parameters from config.yml\n",
                "global_params = global_parameters(config_path='config.yml')\n",
                "\n",
                "# 2. Create a unique, timestamped directory for this experiment run.\n",
                "#    This builds upon the base directory from your config.\n",
                "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
                "run_specific_dir = os.path.join(global_params.base_project_dir, timestamp)\n",
                "pathlib.Path(run_specific_dir).mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# 3. Set up the logger to log into this specific run's directory.\n",
                "logger = setup_logger(log_folder_path=run_specific_dir)\n",
                "\n",
                "logger.info(f\"Experiment outputs will be saved in: {run_specific_dir}\")\n",
                "\n",
                "# 4. Update the global_params object to use this new directory for all outputs.\n",
                "#    This ensures all subsequent file-saving operations (like logs and models)\n",
                "#    go into the correct, unique folder.\n",
                "global_params.base_project_dir = run_specific_dir\n",
                "\n",
                "# init csv to store each local project's results\n",
                "project_score_save_class(global_params.base_project_dir)\n",
                "\n",
                "grid = grid_param_space_ga.Grid(\n",
                "    global_params=global_params,\n",
                "    test_grid=global_params.testing,\n",
                "    config_path='config.yml'\n",
                ")\n",
                "grid_iter_obj = grid.settings_list_iterator\n",
                "\n",
                "for i in tqdm(range(0, global_params.n_iter)):\n",
                "    output.clear_output(wait=True)\n",
                "\n",
                "    # get settings from iterator over grid of settings space\n",
                "    local_param_dict = next(grid_iter_obj)\n",
                "\n",
                "    # Pass the list of model classes to the pipeline\n",
                "    config_dict = {\"modelFuncList\": global_params.model_list}\n",
                "\n",
                "    # create object from settings\n",
                "    ml_grid_object = ml_grid.pipeline.data.pipe(\n",
                "        # input_csv_path: Path to the input CSV file\n",
                "        file_name=global_params.input_csv_path,\n",
                "        \n",
                "        global_params= global_params,\n",
                "        # drop_term_list: List of terms to drop from the data if found in columns (default: empty list)\n",
                "        drop_term_list=[],\n",
                "        # local_param_dict: Dictionary of local parameters (e.g. hyperparameters)\n",
                "        local_param_dict=local_param_dict,\n",
                "        # base_project_dir: Base directory of the project\n",
                "        base_project_dir=global_params.base_project_dir,\n",
                "        # additional_naming: Additional naming convention for the output files\n",
                "        additional_naming='',\n",
                "        # test_sample_n: Number of samples to use for testing (default: 0, i.e. no testing)\n",
                "        test_sample_n=global_params.test_sample_n,\n",
                "        # column_sample_n: Number of columns to sample from the data (default: 0, i.e. all columns)\n",
                "        column_sample_n=global_params.column_sample_n,\n",
                "        # param_space_index: Index of the parameter space to use (e.g. for hyperparameter tuning)\n",
                "        param_space_index=i,\n",
                "        # config_dict: Dictionary of configuration settings\n",
                "        config_dict=config_dict,\n",
                "        # testing: Flag to indicate whether to use the test grid (default: False)\n",
                "        testing=global_params.testing,  # use smaller test grid for GA params\n",
                "        # multiprocessing_ensemble: Flag to indicate whether to use multiprocessing for ensemble methods (default: False)\n",
                "        multiprocessing_ensemble=False\n",
                ")\n",
                "\n",
                "    ml_grid_object.verbose = 0\n",
                "\n",
                "    dummy_generator = DummyModelGenerator(ml_grid_object, local_param_dict)\n",
                "\n",
                "    from ml_grid.pipeline import main_ga\n",
                "\n",
                "    # pass object to be evaluated and write results to csv\n",
                "    res = main_ga.run(ml_grid_object, global_params=global_params,local_param_dict=local_param_dict).execute()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#from ml_grid.util.logger_setup import restore_stdout\n",
                "\n",
                "#restore_stdout()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pd.set_option(\"display.max_columns\", None)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_csv(ml_grid_object.base_project_dir + \"final_grid_score_log.csv\")\n",
                "\n",
                "df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# The original_feature_names should come from the ml_grid_object of the last run\\n\n",
                "original_feature_names = ml_grid_object.original_feature_names"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from ml_grid.util.GA_results_explorer import GA_results_explorer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_csv(ml_grid_object.base_project_dir + \"final_grid_score_log.csv\")\n",
                "\n",
                "df\n",
                "\n",
                "GA_results_explorer = GA_results_explorer(df, original_feature_names)\n",
                "\n",
                "GA_results_explorer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "GA_results_explorer.plot_feature_cooccurrence(performance_metric='auc', plot_dir = ml_grid_object.base_project_dir)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "GA_results_explorer.plot_performance_vs_size(performance_metric='auc', plot_dir = ml_grid_object.base_project_dir)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "GA_results_explorer.plot_algorithm_distribution_in_ensembles(plot_dir = ml_grid_object.base_project_dir)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "GA_results_explorer.plot_feature_stability(performance_metric='auc', top_percent=10.0, feature_type='base_learner', plot_dir = ml_grid_object.base_project_dir)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "GA_results_explorer.plot_all_convergence(history_column='generation_progress_list', performance_metric='auc', highlight_best=True, plot_dir = ml_grid_object.base_project_dir)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "GA_results_explorer.plot_interaction_heatmap(param1='pop_val', param2='run_time', performance_metric='auc', plot_dir = ml_grid_object.base_project_dir)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "GA_results_explorer.plot_performance_tradeoff(performance_metric='auc', cost_metric='run_time', hue_parameter='pop_val', plot_dir = ml_grid_object.base_project_dir)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "GA_results_explorer.plot_ensemble_feature_diversity(outcome_variable='auc', plot_dir = ml_grid_object.base_project_dir)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "GA_results_explorer.plot_parameter_distributions(param_type='run_details', plot_dir = ml_grid_object.base_project_dir)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "GA_results_explorer.plot_parameter_distributions(param_type='config', plot_dir = ml_grid_object.base_project_dir)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "GA_results_explorer.plot_base_learner_feature_importance(outcome_variable='auc', plot_dir = ml_grid_object.base_project_dir)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "GA_results_explorer.plot_initial_feature_importance(outcome_variable='auc', plot_dir = ml_grid_object.base_project_dir)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "GA_results_explorer.plot_combined_anova_feature_importances(outcome_variable='auc', plot_dir = ml_grid_object.base_project_dir)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "GA_results_explorer.plot_run_details_anova_feature_importances(outcome_variable='auc', plot_dir = ml_grid_object.base_project_dir)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "GA_results_explorer.plot_config_anova_feature_importances(outcome_variable='auc', plot_dir = ml_grid_object.base_project_dir)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "assert len(df) > 0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if __name__ == \"__main__\":\n",
                "    from ml_grid.util.evaluate_ensemble_methods import EnsembleEvaluator\n",
                "    import pandas as pd\n",
                "\n",
                "    # Set your paths and parameters\n",
                "    input_csv_path = global_params.input_csv_path  # Path to your input data CSV\n",
                "    results_csv_path = ml_grid_object.base_project_dir + \"final_grid_score_log.csv\"  # Path to your results DataFrame (CSV or PKL)\n",
                "    outcome_variable = \"outcome_var_1\"\n",
                "    initial_param_dict = {\"resample\": None}\n",
                "\n",
                "    try:\n",
                "        evaluator = EnsembleEvaluator(\n",
                "            input_csv_path=input_csv_path,\n",
                "            outcome_variable=outcome_variable,\n",
                "            initial_param_dict=initial_param_dict,\n",
                "            debug=False\n",
                "        )\n",
                "\n",
                "        weighting_methods_to_test = [\"unweighted\", \"de\", \"ann\"]\n",
                "\n",
                "        # Load results DataFrame (try CSV, fallback to pickle)\n",
                "        try:\n",
                "            if results_csv_path.endswith(\".csv\"):\n",
                "                results_df = pd.read_csv(results_csv_path)\n",
                "            elif results_csv_path.endswith(\".pkl\"):\n",
                "                results_df = pd.read_pickle(results_csv_path)\n",
                "        except Exception as e:\n",
                "            logger.error(f\"Could not load results DataFrame: {e}\")\n",
                "            raise\n",
                "\n",
                "        test_results_df = evaluator.evaluate_on_test_set_from_df(\n",
                "            results_df, weighting_methods_to_test\n",
                "        )\n",
                "        logger.info(\"\\n--- Results on TEST SET ---\")\n",
                "        if not test_results_df.empty:\n",
                "            display(test_results_df)\n",
                "\n",
                "        validation_results_df = evaluator.validate_on_holdout_set_from_df(\n",
                "            results_df, weighting_methods_to_test\n",
                "        )\n",
                "        logger.info(\"\\n--- Results on VALIDATION (HOLD-OUT) SET ---\")\n",
                "        if not validation_results_df.empty:\n",
                "            display(validation_results_df)\n",
                "\n",
                "    except (FileNotFoundError, ImportError) as e:\n",
                "        logger.critical(f\"\\nExecution stopped due to a critical error: {e}\")\n",
                "    except Exception as e:\n",
                "        logger.error(f\"\\nAn unexpected error occurred during the evaluation process: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pd.read_csv(ml_grid_object.base_project_dir + \"final_grid_score_log.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.info()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "ga_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
