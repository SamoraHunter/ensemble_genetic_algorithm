{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Function to recursively remove a directory\n",
    "def remove_directory(path):\n",
    "    if os.path.exists(path):\n",
    "        for root, dirs, files in os.walk(path, topdown=False):\n",
    "            for name in files:\n",
    "                os.remove(os.path.join(root, name))\n",
    "            for name in dirs:\n",
    "                os.rmdir(os.path.join(root, name))\n",
    "        os.rmdir(path)\n",
    "        print(f\"Directory '{path}' removed successfully.\")\n",
    "    else:\n",
    "        print(f\"Directory '{path}' does not exist.\")\n",
    "\n",
    "# Specify the directory path to be removed\n",
    "directory_path = 'HFE_GA_experiments'\n",
    "\n",
    "# Remove the directory\n",
    "remove_directory(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable or disable GPU\n",
    "\n",
    "# #os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "# directory = r'HFE_GA_experiments'\n",
    "\n",
    "# if os.path.exists(directory):\n",
    "#     shutil.rmtree(directory)\n",
    "#     print(f\"The directory '{directory}' has been successfully removed.\")\n",
    "# else:\n",
    "#     print(f\"The directory '{directory}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as ipw\n",
    "output = ipw.Output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_grid.util.logger_setup import setup_logger\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger('matplotlib.font_manager')\n",
    "\n",
    "# Set the logging level to suppress debug messages\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Orchestrates a GA grid search and evaluates ensemble models.\n",
    "\n",
    "This script manages a complete pipeline for conducting a genetic algorithm (GA)\n",
    "grid search using the `ml_grid` framework. It automates feature selection,\n",
    "hyperparameter optimization, and the evaluation of various ensemble machine\n",
    "learning models for reproducible experiments.\n",
    "\n",
    "Attributes:\n",
    "    project_directory (str): The global path for saving all experiment outputs.\n",
    "    input_csv_path (str): The file path for the input dataset (CSV format).\n",
    "    n_iter (int): The total number of grid search iterations to perform.\n",
    "    modelFuncList (list): A list of model generator functions that serve as the\n",
    "        base learners for the ensemble methods.\n",
    "\n",
    "Workflow:\n",
    "    1.  **Initialization**:\n",
    "        - Imports necessary modules, including model generators (e.g., logistic\n",
    "          regression, random forest, XGBoost), utilities, and logging tools.\n",
    "        - Establishes a global project directory and creates a unique,\n",
    "          timestamped subdirectory for the current experiment run.\n",
    "        - Initializes a logger to capture and save experiment logs.\n",
    "\n",
    "    2.  **Configuration**:\n",
    "        - Sets the path to the input dataset CSV file.\n",
    "        - Initializes a `project_score_save_class` instance to log the\n",
    "          scores of each experiment iteration to a central CSV file.\n",
    "        - Defines the list of base learners (`modelFuncList`) and the number\n",
    "          of grid search iterations (`n_iter`).\n",
    "        - Instantiates a grid iterator (`grid_iter_obj`) to supply\n",
    "          hyperparameter combinations for each run.\n",
    "\n",
    "    3.  **Main Experiment Loop**:\n",
    "        - Iterates through the specified number of grid search trials.\n",
    "        - In each iteration, it fetches a new set of hyperparameters.\n",
    "        - An `ml_grid_object` is created to handle data loading, preprocessing,\n",
    "          and overall experiment configuration for the current trial.\n",
    "        - The core genetic algorithm pipeline (`main_ga.run().execute()`) is\n",
    "          executed to:\n",
    "            - Evolve ensembles of base learners.\n",
    "            - Evaluate each ensemble against the dataset.\n",
    "            - Log performance metrics and configurations to the experiment\n",
    "              directory.\n",
    "\n",
    "Key Features:\n",
    "    - Supports a wide range of base learners for flexible ensemble creation.\n",
    "    - Fully automates the hyperparameter search and model evaluation process.\n",
    "    - Organizes all results, logs, and artifacts in a structured,\n",
    "      timestamped directory to ensure reproducibility.\n",
    "    - Easily adaptable for both synthetic and real-world datasets.\n",
    "    - Modular design allows for the simple addition of new models or search\n",
    "      parameters.\n",
    "\n",
    "This pipeline is ideal for large-scale, systematic benchmarking of ensemble\n",
    "methods and GA-based feature selection strategies.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ml_grid\n",
    "import pathlib\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from ml_grid.model_classes_ga.dummy_model import DummyModelGenerator\n",
    "from ml_grid.util import grid_param_space_ga\n",
    "\n",
    "import pandas as pd\n",
    "from ml_grid.model_classes_ga.adaboostClassifier_model import (\n",
    "    AdaBoostClassifierModelGenerator,\n",
    ")\n",
    "from ml_grid.model_classes_ga.extra_trees_model import extraTreesModelGenerator\n",
    "from ml_grid.model_classes_ga.gradientBoostingClassifier_model import (\n",
    "    GradientBoostingClassifier_ModelGenerator,\n",
    ")\n",
    "from ml_grid.model_classes_ga.logistic_regression_model import logisticRegressionModelGenerator\n",
    "from ml_grid.model_classes_ga.mlpClassifier_model import MLPClassifier_ModelGenerator\n",
    "from ml_grid.model_classes_ga.perceptron_model import perceptronModelGenerator\n",
    "from ml_grid.model_classes_ga.quadraticDiscriminantAnalysis_model import (\n",
    "    QuadraticDiscriminantAnalysis_ModelGenerator,\n",
    ")\n",
    "from ml_grid.model_classes_ga.randomForest_model import randomForestModelGenerator\n",
    "from ml_grid.model_classes_ga.svc_model import SVC_ModelGenerator\n",
    "from ml_grid.model_classes_ga.gaussianNB_model import GaussianNB_ModelGenerator\n",
    "from ml_grid.model_classes_ga.kNearestNeighbors_model import (\n",
    "    kNearestNeighborsModelGenerator,\n",
    ")\n",
    "from ml_grid.model_classes_ga.elasticNeuralNetwork_model import (\n",
    "    elasticNeuralNetworkModelGenerator,\n",
    ")\n",
    "from ml_grid.model_classes_ga.XGBoost_model import XGBoostModelGenerator\n",
    "from ml_grid.model_classes_ga.decisionTreeClassifier_model import (\n",
    "    DecisionTreeClassifierModelGenerator,\n",
    ")\n",
    "from ml_grid.model_classes_ga.pytorchANNBinaryClassifier_model import (\n",
    "    Pytorch_binary_class_ModelGenerator,\n",
    ")\n",
    "\n",
    "\n",
    "from ml_grid.util.project_score_save import project_score_save_class\n",
    "\n",
    "base_project_dir_global = \"HFE_GA_experiments/\"\n",
    "\n",
    "logger = setup_logger(log_folder_path = base_project_dir_global)\n",
    "\n",
    "pathlib.Path(base_project_dir_global).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "st_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%I-%M-%S_%p\")\n",
    "\n",
    "base_project_dir = \"HFE_GA_experiments/\" + st_time + \"/\"\n",
    "additional_naming = \"HFE_GA_Grid_\"\n",
    "\n",
    "print(base_project_dir)\n",
    "\n",
    "pathlib.Path(base_project_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# input_csv_path = '/home/aliencat/samora/HFE/HFE/v20/30163_to_16408_imputed_outcome_grid.csv'\n",
    "\n",
    "input_csv_path = (\n",
    "    \"/home/aliencat/samora/HFE/HFE/v22/hfe_TC_merge_T_Im_10k_1yr_mean_imputed.csv\"\n",
    ")\n",
    "\n",
    "input_csv_path = \"synthetic_data_for_testing.csv\"\n",
    "\n",
    "#input_csv_path = \"synthetic_sample_100_features_4.csv\"\n",
    "\n",
    "# init csv to store each local projects results\n",
    "\n",
    "project_score_save_class(base_project_dir)\n",
    "\n",
    "n_iter = 3000 # Number of iterations over the settings list (see the Grid in grid_param_space_ga.Grid)\n",
    "\n",
    "grid_iter_obj = grid_param_space_ga.Grid(sample_n=n_iter).settings_list_iterator\n",
    "\n",
    "#dummy_generator = DummyModelGenerator(ml_grid_object, local_param_dict)\n",
    "\n",
    "modelFuncList = [\n",
    "    #         dummy_model_gen,\n",
    "    #         dummy_model_gen,\n",
    "    #dummy_generator.dummy_model_gen,\n",
    "    logisticRegressionModelGenerator,\n",
    "    perceptronModelGenerator,\n",
    "    extraTreesModelGenerator,\n",
    "    randomForestModelGenerator,\n",
    "    kNearestNeighborsModelGenerator,\n",
    "    XGBoostModelGenerator,\n",
    "    DecisionTreeClassifierModelGenerator,\n",
    "    AdaBoostClassifierModelGenerator,\n",
    "    elasticNeuralNetworkModelGenerator,\n",
    "    GaussianNB_ModelGenerator,\n",
    "    QuadraticDiscriminantAnalysis_ModelGenerator,\n",
    "    SVC_ModelGenerator,\n",
    "    GradientBoostingClassifier_ModelGenerator,\n",
    "    MLPClassifier_ModelGenerator,\n",
    "    # #     #KerasClassifier_ModelGen()\n",
    "    Pytorch_binary_class_ModelGenerator,\n",
    "]\n",
    "\n",
    "\n",
    "config_dict = {\n",
    "    \"use_stored_base_learners\": False,\n",
    "    \"modelFuncList\": modelFuncList,\n",
    "}\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, n_iter)):\n",
    "    output.clear_output(wait=True)\n",
    "\n",
    "    # get settings from iterator over grid of settings space\n",
    "    local_param_dict = next(grid_iter_obj)\n",
    "\n",
    "    # create object from settings\n",
    "    ml_grid_object = ml_grid.pipeline.data.pipe(\n",
    "    # input_csv_path: Path to the input CSV file\n",
    "    input_csv_path,\n",
    "    \n",
    "    # drop_term_list: List of terms to drop from the data (default: empty list)\n",
    "    drop_term_list=[],\n",
    "    \n",
    "    # local_param_dict: Dictionary of local parameters (e.g. hyperparameters)\n",
    "    local_param_dict=local_param_dict,\n",
    "    \n",
    "    # base_project_dir: Base directory of the project\n",
    "    base_project_dir=base_project_dir,\n",
    "    \n",
    "    # additional_naming: Additional naming convention for the output files\n",
    "    additional_naming=additional_naming,\n",
    "    \n",
    "    # test_sample_n: Number of samples to use for testing (default: 0, i.e. no testing)\n",
    "    test_sample_n=500,\n",
    "    \n",
    "    # column_sample_n: Number of columns to sample from the data (default: 0, i.e. all columns)\n",
    "    column_sample_n=30,\n",
    "    \n",
    "    # param_space_index: Index of the parameter space to use (e.g. for hyperparameter tuning)\n",
    "    param_space_index=i,\n",
    "    \n",
    "    # config_dict: Dictionary of configuration settings\n",
    "    config_dict=config_dict,\n",
    "    \n",
    "    # testing: Flag to indicate whether to use the test grid (default: False)\n",
    "    testing=True,  # use test grid\n",
    "    \n",
    "    # multiprocessing_ensemble: Flag to indicate whether to use multiprocessing for ensemble methods (default: False)\n",
    "    multiprocessing_ensemble=False\n",
    ")\n",
    "\n",
    "    ml_grid_object.verbose = 0\n",
    "\n",
    "    dummy_generator = DummyModelGenerator(ml_grid_object, local_param_dict)\n",
    "\n",
    "    #modelFuncList.append(dummy_generator.dummy_model_gen) # Use for testing only.\n",
    "\n",
    "    use_stored_base_learners = local_param_dict.get(\"use_stored_base_learners\")\n",
    "\n",
    "    store_base_learners = local_param_dict.get(\"store_base_learners\")\n",
    "\n",
    "    from ml_grid.pipeline import main_ga\n",
    "\n",
    "    # pass object to be evaluated and write results to csv\n",
    "    res = main_ga.run(ml_grid_object, local_param_dict=local_param_dict).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_grid.util.logger_setup import restore_stdout\n",
    "\n",
    "restore_stdout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(ml_grid_object.base_project_dir + \"final_grid_score_log.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_feature_names = pd.read_csv(input_csv_path).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_grid.util.GA_results_explorer import GA_results_explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(ml_grid_object.base_project_dir + \"final_grid_score_log.csv\")\n",
    "\n",
    "df\n",
    "\n",
    "GA_results_explorer = GA_results_explorer(df, original_feature_names)\n",
    "\n",
    "GA_results_explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GA_results_explorer.plot_feature_cooccurrence(performance_metric='auc', plot_dir = ml_grid_object.base_project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GA_results_explorer.plot_performance_vs_size(performance_metric='auc', plot_dir = ml_grid_object.base_project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GA_results_explorer.plot_algorithm_distribution_in_ensembles(plot_dir = ml_grid_object.base_project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GA_results_explorer.plot_feature_stability(performance_metric='auc', top_percent=10.0, feature_type='base_learner', plot_dir = ml_grid_object.base_project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GA_results_explorer.plot_all_convergence(history_column='generation_progress_list', performance_metric='auc', highlight_best=True, plot_dir = ml_grid_object.base_project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GA_results_explorer.plot_interaction_heatmap(param1='pop_val', param2='run_time', performance_metric='auc', plot_dir = ml_grid_object.base_project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GA_results_explorer.plot_performance_tradeoff(performance_metric='auc', cost_metric='run_time', hue_parameter='pop_val', plot_dir = ml_grid_object.base_project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GA_results_explorer.plot_ensemble_feature_diversity(outcome_variable='auc', plot_dir = ml_grid_object.base_project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GA_results_explorer.plot_parameter_distributions(param_type='run_details', plot_dir = ml_grid_object.base_project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GA_results_explorer.plot_parameter_distributions(param_type='config', plot_dir = ml_grid_object.base_project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GA_results_explorer.plot_base_learner_feature_importance(outcome_variable='auc', plot_dir = ml_grid_object.base_project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GA_results_explorer.plot_initial_feature_importance(outcome_variable='auc', plot_dir = ml_grid_object.base_project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GA_results_explorer.plot_combined_anova_feature_importances(outcome_variable='auc', plot_dir = ml_grid_object.base_project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GA_results_explorer.plot_run_details_anova_feature_importances(outcome_variable='auc', plot_dir = ml_grid_object.base_project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GA_results_explorer.plot_config_anova_feature_importances(outcome_variable='auc', plot_dir = ml_grid_object.base_project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from ml_grid.util.evaluate_ensemble_methods import EnsembleEvaluator\n",
    "    import pandas as pd\n",
    "\n",
    "    # Set your paths and parameters\n",
    "    input_csv_path = input_csv_path  # Path to your input data CSV\n",
    "    results_csv_path = ml_grid_object.base_project_dir + \"final_grid_score_log.csv\"  # Path to your results DataFrame (CSV or PKL)\n",
    "    outcome_variable = \"outcome_var_1\"\n",
    "    initial_param_dict = {\"resample\": None}\n",
    "\n",
    "    try:\n",
    "        evaluator = EnsembleEvaluator(\n",
    "            input_csv_path=input_csv_path,\n",
    "            outcome_variable=outcome_variable,\n",
    "            initial_param_dict=initial_param_dict,\n",
    "            debug=False\n",
    "        )\n",
    "\n",
    "        weighting_methods_to_test = [\"unweighted\", \"de\", \"ann\"]\n",
    "\n",
    "        # Load results DataFrame (try CSV, fallback to pickle)\n",
    "        try:\n",
    "            if results_csv_path.endswith(\".csv\"):\n",
    "                results_df = pd.read_csv(results_csv_path)\n",
    "            else:\n",
    "                results_df = pd.read_pickle(results_csv_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load results DataFrame: {e}\")\n",
    "            raise\n",
    "\n",
    "        test_results_df = evaluator.evaluate_on_test_set_from_df(\n",
    "            results_df, weighting_methods_to_test\n",
    "        )\n",
    "        print(\"\\n--- Results on TEST SET ---\")\n",
    "        if not test_results_df.empty:\n",
    "            display(test_results_df)\n",
    "\n",
    "        validation_results_df = evaluator.validate_on_holdout_set_from_df(\n",
    "            results_df, weighting_methods_to_test\n",
    "        )\n",
    "        print(\"\\n--- Results on VALIDATION (HOLD-OUT) SET ---\")\n",
    "        if not validation_results_df.empty:\n",
    "            display(validation_results_df)\n",
    "\n",
    "    except (FileNotFoundError, ImportError) as e:\n",
    "        print(f\"\\nExecution stopped due to a critical error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during the evaluation process: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(ml_grid_object.base_project_dir + \"final_grid_score_log.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ga_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
